{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Display\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, classIdx, layerName=None):\n",
    "        # store the model, the class index used to measure the class\n",
    "        # activation map, and the layer to be used when visualizing\n",
    "        # the class activation map\n",
    "        self.model = model\n",
    "        self.classIdx = classIdx\n",
    "        self.layerName = layerName\n",
    "        # if the layer name is None, attempt to automatically find\n",
    "        # the target output layer\n",
    "        if self.layerName is None:\n",
    "            self.layerName = self.find_target_layer()\n",
    "\n",
    "    def find_target_layer(self):\n",
    "        # attempt to find the final convolutional layer in the network\n",
    "        # by looping over the layers of the network in reverse order\n",
    "        for layer in reversed(self.model.layers):\n",
    "            # check to see if the layer has a 4D output\n",
    "            if len(layer.output_shape) == 4:\n",
    "                return layer.name\n",
    "        # otherwise, we could not find a 4D layer so the GradCAM\n",
    "        # algorithm cannot be applied\n",
    "        raise ValueError(\"Could not find 4D layer. Cannot apply GradCAM.\")\n",
    "\n",
    "\n",
    "    def compute_heatmap(self, image, eps=1e-8):\n",
    "        # construct our gradient model by supplying (1) the inputs\n",
    "        # to our pre-trained model, (2) the output of the (presumably)\n",
    "        # final 4D layer in the network, and (3) the output of the\n",
    "        # softmax activations from the model\n",
    "        gradModel = Model(\n",
    "            inputs=[self.model.inputs],\n",
    "            outputs=[self.model.get_layer(self.layerName).output, self.model.output])\n",
    "\n",
    "        # record operations for automatic differentiation\n",
    "        with tf.GradientTape() as tape:\n",
    "            # cast the image tensor to a float-32 data type, pass the\n",
    "            # image through the gradient model, and grab the loss\n",
    "            # associated with the specific class index\n",
    "            inputs = tf.cast(image, tf.float32)\n",
    "            (convOutputs, predictions) = gradModel(inputs)\n",
    "            \n",
    "            loss = predictions[:, tf.argmax(predictions[0])]\n",
    "    \n",
    "        # use automatic differentiation to compute the gradients\n",
    "        grads = tape.gradient(loss, convOutputs)\n",
    "\n",
    "        # compute the guided gradients\n",
    "        castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n",
    "        castGrads = tf.cast(grads > 0, \"float32\")\n",
    "        guidedGrads = castConvOutputs * castGrads * grads\n",
    "        # the convolution and guided gradients have a batch dimension\n",
    "        # (which we don't need) so let's grab the volume itself and\n",
    "        # discard the batch\n",
    "        convOutputs = convOutputs[0]\n",
    "        guidedGrads = guidedGrads[0]\n",
    "\n",
    "        # compute the average of the gradient values, and using them\n",
    "        # as weights, compute the ponderation of the filters with\n",
    "        # respect to the weights\n",
    "        weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n",
    "        cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n",
    "\n",
    "        # grab the spatial dimensions of the input image and resize\n",
    "        # the output class activation map to match the input image\n",
    "        # dimensions\n",
    "        (w, h) = (image.shape[2], image.shape[1])\n",
    "        heatmap = cv2.resize(cam.numpy(), (w, h))\n",
    "        # normalize the heatmap such that all values lie in the range\n",
    "        # [0, 1], scale the resulting values to the range [0, 255],\n",
    "        # and then convert to an unsigned 8-bit integer\n",
    "        numer = heatmap - np.min(heatmap)\n",
    "        denom = (heatmap.max() - heatmap.min()) + eps\n",
    "        heatmap = numer / denom\n",
    "        heatmap = (heatmap * 255).astype(\"uint8\")\n",
    "        # return the resulting heatmap to the calling function\n",
    "        return heatmap\n",
    "\n",
    "    def overlay_heatmap(self, heatmap, image, alpha=0.5,\n",
    "                        colormap=cv2.COLORMAP_VIRIDIS):\n",
    "        # apply the supplied color map to the heatmap and then\n",
    "        # overlay the heatmap on the input image\n",
    "        heatmap = cv2.applyColorMap(heatmap, colormap)\n",
    "        output = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n",
    "        # return a 2-tuple of the color mapped heatmap and the output,\n",
    "        # overlaid image\n",
    "        return (heatmap, output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import pickle\n",
    "with open('train.pickle', 'rb') as f:\n",
    "    X, y = pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# train set / data \n",
    "x_train = X_train.astype('float32') / 255\n",
    "# train set / target \n",
    "y_train = tf.keras.utils.to_categorical(y_train , num_classes=7)\n",
    "\n",
    "# validation set / data \n",
    "x_test = X_test.astype('float32') / 255\n",
    "# validation set / target \n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=7)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)  "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(6395, 512, 512) (6395, 512, 512, 7)\n",
      "(3151, 512, 512) (3151, 512, 512, 7)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "smooth = 1.\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "  y_true = tf.cast(y_true, tf.float32)\n",
    "  y_pred = tf.math.softmax(y_pred)\n",
    "  numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "  denominator = tf.reduce_sum(y_true + y_pred)\n",
    "\n",
    "  return 1 - numerator / denominator\n",
    "\n",
    "path_to_model = '/Users/salvatoreesposito/Desktop/models/AttXnet_1'\n",
    "model = tf.keras.models.load_model(path_to_model, custom_objects={'dice_loss':dice_loss, \"dice_coef\":dice_coef})"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "model.summary()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"AttU_enc\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 512, 512, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage0-0_conv1 (Conv2D) (None, 512, 512, 32) 288         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage0-0_bn1 (BatchNorm (None, 512, 512, 32) 128         encoder_stage0-0_conv1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage0-0_relu1 (Activat (None, 512, 512, 32) 0           encoder_stage0-0_bn1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage0-0_conv2 (Conv2D) (None, 512, 512, 32) 9216        encoder_stage0-0_relu1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage0-0_bn2 (BatchNorm (None, 512, 512, 32) 128         encoder_stage0-0_conv2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage0-0_relu2 (Activat (None, 512, 512, 32) 0           encoder_stage0-0_bn2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage1-0_maxpool (MaxPo (None, 256, 256, 32) 0           encoder_stage0-0_relu2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage1-0_conv1 (Conv2D) (None, 256, 256, 64) 18432       encoder_stage1-0_maxpool[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage1-0_bn1 (BatchNorm (None, 256, 256, 64) 256         encoder_stage1-0_conv1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage1-0_relu1 (Activat (None, 256, 256, 64) 0           encoder_stage1-0_bn1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage1-0_conv2 (Conv2D) (None, 256, 256, 64) 36864       encoder_stage1-0_relu1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage1-0_bn2 (BatchNorm (None, 256, 256, 64) 256         encoder_stage1-0_conv2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage1-0_relu2 (Activat (None, 256, 256, 64) 0           encoder_stage1-0_bn2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage2-0_maxpool (MaxPo (None, 128, 128, 64) 0           encoder_stage1-0_relu2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage2-0_conv1 (Conv2D) (None, 128, 128, 128 73728       encoder_stage2-0_maxpool[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage2-0_bn1 (BatchNorm (None, 128, 128, 128 512         encoder_stage2-0_conv1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage2-0_relu1 (Activat (None, 128, 128, 128 0           encoder_stage2-0_bn1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage2-0_conv2 (Conv2D) (None, 128, 128, 128 147456      encoder_stage2-0_relu1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage2-0_bn2 (BatchNorm (None, 128, 128, 128 512         encoder_stage2-0_conv2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage2-0_relu2 (Activat (None, 128, 128, 128 0           encoder_stage2-0_bn2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage3-0_maxpool (MaxPo (None, 64, 64, 128)  0           encoder_stage2-0_relu2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage3-0_conv1 (Conv2D) (None, 64, 64, 256)  294912      encoder_stage3-0_maxpool[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage3-0_bn1 (BatchNorm (None, 64, 64, 256)  1024        encoder_stage3-0_conv1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage3-0_relu1 (Activat (None, 64, 64, 256)  0           encoder_stage3-0_bn1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage3-0_conv2 (Conv2D) (None, 64, 64, 256)  589824      encoder_stage3-0_relu1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage3-0_bn2 (BatchNorm (None, 64, 64, 256)  1024        encoder_stage3-0_conv2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage3-0_relu2 (Activat (None, 64, 64, 256)  0           encoder_stage3-0_bn2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage4-0_maxpool (MaxPo (None, 32, 32, 256)  0           encoder_stage3-0_relu2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage4-0_conv1 (Conv2D) (None, 32, 32, 512)  1179648     encoder_stage4-0_maxpool[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage4-0_bn1 (BatchNorm (None, 32, 32, 512)  2048        encoder_stage4-0_conv1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage4-0_relu1 (Activat (None, 32, 32, 512)  0           encoder_stage4-0_bn1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage4-0_conv2 (Conv2D) (None, 32, 32, 512)  2359296     encoder_stage4-0_relu1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage4-0_bn2 (BatchNorm (None, 32, 32, 512)  2048        encoder_stage4-0_conv2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_stage4-0_relu2 (Activat (None, 32, 32, 512)  0           encoder_stage4-0_bn2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage3-1_upsample_bef (None, 64, 64, 512)  0           encoder_stage4-0_relu2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage3-1_conv_before  (None, 64, 64, 256)  1179648     attention_stage3-1_upsample_befor\n",
      "__________________________________________________________________________________________________\n",
      "attention_stage3-1_bn_before (B (None, 64, 64, 256)  1024        attention_stage3-1_conv_before[0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_stage3-1_relu_before  (None, 64, 64, 256)  0           attention_stage3-1_bn_before[0][0\n",
      "__________________________________________________________________________________________________\n",
      "attention_stage3-1_conv_skip (C (None, 64, 64, 256)  65792       encoder_stage3-0_relu2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage3-1_conv_up (Con (None, 64, 64, 256)  65792       attention_stage3-1_relu_before[0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_stage3-1_bn1 (BatchNo (None, 64, 64, 256)  1024        attention_stage3-1_conv_skip[0][0\n",
      "__________________________________________________________________________________________________\n",
      "attention_stage3-1_bn2 (BatchNo (None, 64, 64, 256)  1024        attention_stage3-1_conv_up[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage3-1_add (Add)    (None, 64, 64, 256)  0           attention_stage3-1_bn1[0][0]     \n",
      "                                                                 attention_stage3-1_bn2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage3-1_relu (Activa (None, 64, 64, 256)  0           attention_stage3-1_add[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage3-1_conv (Conv2D (None, 64, 64, 1)    257         attention_stage3-1_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage3-1_bn3 (BatchNo (None, 64, 64, 1)    4           attention_stage3-1_conv[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage3-1_sigmoid (Act (None, 64, 64, 1)    0           attention_stage3-1_bn3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3-1_upsample (UpSa (None, 64, 64, 512)  0           encoder_stage4-0_relu2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage3-1_mul (Multipl (None, 64, 64, 256)  0           encoder_stage3-0_relu2[0][0]     \n",
      "                                                                 attention_stage3-1_sigmoid[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "merge_3-1 (Concatenate)         (None, 64, 64, 768)  0           decoder_stage3-1_upsample[0][0]  \n",
      "                                                                 attention_stage3-1_mul[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3-1_conv1 (Conv2D) (None, 64, 64, 256)  1769472     merge_3-1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3-1_bn1 (BatchNorm (None, 64, 64, 256)  1024        decoder_stage3-1_conv1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3-1_relu1 (Activat (None, 64, 64, 256)  0           decoder_stage3-1_bn1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3-1_conv2 (Conv2D) (None, 64, 64, 256)  589824      decoder_stage3-1_relu1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3-1_bn2 (BatchNorm (None, 64, 64, 256)  1024        decoder_stage3-1_conv2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3-1_relu2 (Activat (None, 64, 64, 256)  0           decoder_stage3-1_bn2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage2-2_upsample_bef (None, 128, 128, 256 0           decoder_stage3-1_relu2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage2-2_conv_before  (None, 128, 128, 128 294912      attention_stage2-2_upsample_befor\n",
      "__________________________________________________________________________________________________\n",
      "attention_stage2-2_bn_before (B (None, 128, 128, 128 512         attention_stage2-2_conv_before[0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_stage2-2_relu_before  (None, 128, 128, 128 0           attention_stage2-2_bn_before[0][0\n",
      "__________________________________________________________________________________________________\n",
      "attention_stage2-2_conv_skip (C (None, 128, 128, 128 16512       encoder_stage2-0_relu2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage2-2_conv_up (Con (None, 128, 128, 128 16512       attention_stage2-2_relu_before[0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_stage2-2_bn1 (BatchNo (None, 128, 128, 128 512         attention_stage2-2_conv_skip[0][0\n",
      "__________________________________________________________________________________________________\n",
      "attention_stage2-2_bn2 (BatchNo (None, 128, 128, 128 512         attention_stage2-2_conv_up[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage2-2_add (Add)    (None, 128, 128, 128 0           attention_stage2-2_bn1[0][0]     \n",
      "                                                                 attention_stage2-2_bn2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage2-2_relu (Activa (None, 128, 128, 128 0           attention_stage2-2_add[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage2-2_conv (Conv2D (None, 128, 128, 1)  129         attention_stage2-2_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage2-2_bn3 (BatchNo (None, 128, 128, 1)  4           attention_stage2-2_conv[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage2-2_sigmoid (Act (None, 128, 128, 1)  0           attention_stage2-2_bn3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2-2_upsample (UpSa (None, 128, 128, 256 0           decoder_stage3-1_relu2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage2-2_mul (Multipl (None, 128, 128, 128 0           encoder_stage2-0_relu2[0][0]     \n",
      "                                                                 attention_stage2-2_sigmoid[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "merge_2-2 (Concatenate)         (None, 128, 128, 384 0           decoder_stage2-2_upsample[0][0]  \n",
      "                                                                 attention_stage2-2_mul[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2-2_conv1 (Conv2D) (None, 128, 128, 128 442368      merge_2-2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2-2_bn1 (BatchNorm (None, 128, 128, 128 512         decoder_stage2-2_conv1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2-2_relu1 (Activat (None, 128, 128, 128 0           decoder_stage2-2_bn1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2-2_conv2 (Conv2D) (None, 128, 128, 128 147456      decoder_stage2-2_relu1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2-2_bn2 (BatchNorm (None, 128, 128, 128 512         decoder_stage2-2_conv2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2-2_relu2 (Activat (None, 128, 128, 128 0           decoder_stage2-2_bn2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage1-3_upsample_bef (None, 256, 256, 128 0           decoder_stage2-2_relu2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage1-3_conv_before  (None, 256, 256, 64) 73728       attention_stage1-3_upsample_befor\n",
      "__________________________________________________________________________________________________\n",
      "attention_stage1-3_bn_before (B (None, 256, 256, 64) 256         attention_stage1-3_conv_before[0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_stage1-3_relu_before  (None, 256, 256, 64) 0           attention_stage1-3_bn_before[0][0\n",
      "__________________________________________________________________________________________________\n",
      "attention_stage1-3_conv_skip (C (None, 256, 256, 64) 4160        encoder_stage1-0_relu2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage1-3_conv_up (Con (None, 256, 256, 64) 4160        attention_stage1-3_relu_before[0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_stage1-3_bn1 (BatchNo (None, 256, 256, 64) 256         attention_stage1-3_conv_skip[0][0\n",
      "__________________________________________________________________________________________________\n",
      "attention_stage1-3_bn2 (BatchNo (None, 256, 256, 64) 256         attention_stage1-3_conv_up[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage1-3_add (Add)    (None, 256, 256, 64) 0           attention_stage1-3_bn1[0][0]     \n",
      "                                                                 attention_stage1-3_bn2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage1-3_relu (Activa (None, 256, 256, 64) 0           attention_stage1-3_add[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage1-3_conv (Conv2D (None, 256, 256, 1)  65          attention_stage1-3_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage1-3_bn3 (BatchNo (None, 256, 256, 1)  4           attention_stage1-3_conv[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage1-3_sigmoid (Act (None, 256, 256, 1)  0           attention_stage1-3_bn3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1-3_upsample (UpSa (None, 256, 256, 128 0           decoder_stage2-2_relu2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage1-3_mul (Multipl (None, 256, 256, 64) 0           encoder_stage1-0_relu2[0][0]     \n",
      "                                                                 attention_stage1-3_sigmoid[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "merge_1-3 (Concatenate)         (None, 256, 256, 192 0           decoder_stage1-3_upsample[0][0]  \n",
      "                                                                 attention_stage1-3_mul[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1-3_conv1 (Conv2D) (None, 256, 256, 64) 110592      merge_1-3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1-3_bn1 (BatchNorm (None, 256, 256, 64) 256         decoder_stage1-3_conv1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1-3_relu1 (Activat (None, 256, 256, 64) 0           decoder_stage1-3_bn1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1-3_conv2 (Conv2D) (None, 256, 256, 64) 36864       decoder_stage1-3_relu1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1-3_bn2 (BatchNorm (None, 256, 256, 64) 256         decoder_stage1-3_conv2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1-3_relu2 (Activat (None, 256, 256, 64) 0           decoder_stage1-3_bn2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage0-4_upsample_bef (None, 512, 512, 64) 0           decoder_stage1-3_relu2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage0-4_conv_before  (None, 512, 512, 32) 18432       attention_stage0-4_upsample_befor\n",
      "__________________________________________________________________________________________________\n",
      "attention_stage0-4_bn_before (B (None, 512, 512, 32) 128         attention_stage0-4_conv_before[0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_stage0-4_relu_before  (None, 512, 512, 32) 0           attention_stage0-4_bn_before[0][0\n",
      "__________________________________________________________________________________________________\n",
      "attention_stage0-4_conv_skip (C (None, 512, 512, 32) 1056        encoder_stage0-0_relu2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage0-4_conv_up (Con (None, 512, 512, 32) 1056        attention_stage0-4_relu_before[0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_stage0-4_bn1 (BatchNo (None, 512, 512, 32) 128         attention_stage0-4_conv_skip[0][0\n",
      "__________________________________________________________________________________________________\n",
      "attention_stage0-4_bn2 (BatchNo (None, 512, 512, 32) 128         attention_stage0-4_conv_up[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage0-4_add (Add)    (None, 512, 512, 32) 0           attention_stage0-4_bn1[0][0]     \n",
      "                                                                 attention_stage0-4_bn2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage0-4_relu (Activa (None, 512, 512, 32) 0           attention_stage0-4_add[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage0-4_conv (Conv2D (None, 512, 512, 1)  33          attention_stage0-4_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage0-4_bn3 (BatchNo (None, 512, 512, 1)  4           attention_stage0-4_conv[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage0-4_sigmoid (Act (None, 512, 512, 1)  0           attention_stage0-4_bn3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0-4_upsample (UpSa (None, 512, 512, 64) 0           decoder_stage1-3_relu2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "attention_stage0-4_mul (Multipl (None, 512, 512, 32) 0           encoder_stage0-0_relu2[0][0]     \n",
      "                                                                 attention_stage0-4_sigmoid[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "merge_0-4 (Concatenate)         (None, 512, 512, 96) 0           decoder_stage0-4_upsample[0][0]  \n",
      "                                                                 attention_stage0-4_mul[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0-4_conv1 (Conv2D) (None, 512, 512, 32) 27648       merge_0-4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0-4_bn1 (BatchNorm (None, 512, 512, 32) 128         decoder_stage0-4_conv1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0-4_relu1 (Activat (None, 512, 512, 32) 0           decoder_stage0-4_bn1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0-4_conv2 (Conv2D) (None, 512, 512, 32) 9216        decoder_stage0-4_relu1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0-4_bn2 (BatchNorm (None, 512, 512, 32) 128         decoder_stage0-4_conv2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0-4_relu2 (Activat (None, 512, 512, 32) 0           decoder_stage0-4_bn2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "final_conv (Conv2D)             (None, 512, 512, 7)  2023        decoder_stage0-4_relu2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Activation)            (None, 512, 512, 7)  0           final_conv[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 9,604,923\n",
      "Trainable params: 9,596,147\n",
      "Non-trainable params: 8,776\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "import imageio\n",
    "image = imageio.imread(\"greyscale2.jpg\", as_gray =True)\n",
    "image = image.astype('float32') / 255\n",
    "# image = np.expand_dims(image, axis=0)\n",
    "\n",
    "preds=model.predict(np.expand_dims(image, axis=0))\n",
    "i = np.argmax(preds[0])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1831396"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "for idx in range(len(model.layers)):\n",
    "  print(model.get_layer(index = idx).name)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input_1\n",
      "encoder_stage0-0_conv1\n",
      "encoder_stage0-0_bn1\n",
      "encoder_stage0-0_relu1\n",
      "encoder_stage0-0_conv2\n",
      "encoder_stage0-0_bn2\n",
      "encoder_stage0-0_relu2\n",
      "encoder_stage1-0_maxpool\n",
      "encoder_stage1-0_conv1\n",
      "encoder_stage1-0_bn1\n",
      "encoder_stage1-0_relu1\n",
      "encoder_stage1-0_conv2\n",
      "encoder_stage1-0_bn2\n",
      "encoder_stage1-0_relu2\n",
      "encoder_stage2-0_maxpool\n",
      "encoder_stage2-0_conv1\n",
      "encoder_stage2-0_bn1\n",
      "encoder_stage2-0_relu1\n",
      "encoder_stage2-0_conv2\n",
      "encoder_stage2-0_bn2\n",
      "encoder_stage2-0_relu2\n",
      "encoder_stage3-0_maxpool\n",
      "encoder_stage3-0_conv1\n",
      "encoder_stage3-0_bn1\n",
      "encoder_stage3-0_relu1\n",
      "encoder_stage3-0_conv2\n",
      "encoder_stage3-0_bn2\n",
      "encoder_stage3-0_relu2\n",
      "encoder_stage4-0_maxpool\n",
      "encoder_stage4-0_conv1\n",
      "encoder_stage4-0_bn1\n",
      "encoder_stage4-0_relu1\n",
      "encoder_stage4-0_conv2\n",
      "encoder_stage4-0_bn2\n",
      "encoder_stage4-0_relu2\n",
      "attention_stage3-1_upsample_before\n",
      "attention_stage3-1_conv_before\n",
      "attention_stage3-1_bn_before\n",
      "attention_stage3-1_relu_before\n",
      "attention_stage3-1_conv_skip\n",
      "attention_stage3-1_conv_up\n",
      "attention_stage3-1_bn1\n",
      "attention_stage3-1_bn2\n",
      "attention_stage3-1_add\n",
      "attention_stage3-1_relu\n",
      "attention_stage3-1_conv\n",
      "attention_stage3-1_bn3\n",
      "attention_stage3-1_sigmoid\n",
      "decoder_stage3-1_upsample\n",
      "attention_stage3-1_mul\n",
      "merge_3-1\n",
      "decoder_stage3-1_conv1\n",
      "decoder_stage3-1_bn1\n",
      "decoder_stage3-1_relu1\n",
      "decoder_stage3-1_conv2\n",
      "decoder_stage3-1_bn2\n",
      "decoder_stage3-1_relu2\n",
      "attention_stage2-2_upsample_before\n",
      "attention_stage2-2_conv_before\n",
      "attention_stage2-2_bn_before\n",
      "attention_stage2-2_relu_before\n",
      "attention_stage2-2_conv_skip\n",
      "attention_stage2-2_conv_up\n",
      "attention_stage2-2_bn1\n",
      "attention_stage2-2_bn2\n",
      "attention_stage2-2_add\n",
      "attention_stage2-2_relu\n",
      "attention_stage2-2_conv\n",
      "attention_stage2-2_bn3\n",
      "attention_stage2-2_sigmoid\n",
      "decoder_stage2-2_upsample\n",
      "attention_stage2-2_mul\n",
      "merge_2-2\n",
      "decoder_stage2-2_conv1\n",
      "decoder_stage2-2_bn1\n",
      "decoder_stage2-2_relu1\n",
      "decoder_stage2-2_conv2\n",
      "decoder_stage2-2_bn2\n",
      "decoder_stage2-2_relu2\n",
      "attention_stage1-3_upsample_before\n",
      "attention_stage1-3_conv_before\n",
      "attention_stage1-3_bn_before\n",
      "attention_stage1-3_relu_before\n",
      "attention_stage1-3_conv_skip\n",
      "attention_stage1-3_conv_up\n",
      "attention_stage1-3_bn1\n",
      "attention_stage1-3_bn2\n",
      "attention_stage1-3_add\n",
      "attention_stage1-3_relu\n",
      "attention_stage1-3_conv\n",
      "attention_stage1-3_bn3\n",
      "attention_stage1-3_sigmoid\n",
      "decoder_stage1-3_upsample\n",
      "attention_stage1-3_mul\n",
      "merge_1-3\n",
      "decoder_stage1-3_conv1\n",
      "decoder_stage1-3_bn1\n",
      "decoder_stage1-3_relu1\n",
      "decoder_stage1-3_conv2\n",
      "decoder_stage1-3_bn2\n",
      "decoder_stage1-3_relu2\n",
      "attention_stage0-4_upsample_before\n",
      "attention_stage0-4_conv_before\n",
      "attention_stage0-4_bn_before\n",
      "attention_stage0-4_relu_before\n",
      "attention_stage0-4_conv_skip\n",
      "attention_stage0-4_conv_up\n",
      "attention_stage0-4_bn1\n",
      "attention_stage0-4_bn2\n",
      "attention_stage0-4_add\n",
      "attention_stage0-4_relu\n",
      "attention_stage0-4_conv\n",
      "attention_stage0-4_bn3\n",
      "attention_stage0-4_sigmoid\n",
      "decoder_stage0-4_upsample\n",
      "attention_stage0-4_mul\n",
      "merge_0-4\n",
      "decoder_stage0-4_conv1\n",
      "decoder_stage0-4_bn1\n",
      "decoder_stage0-4_relu1\n",
      "decoder_stage0-4_conv2\n",
      "decoder_stage0-4_bn2\n",
      "decoder_stage0-4_relu2\n",
      "final_conv\n",
      "softmax\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "icam = GradCAM(model, i, 'encoder_stage1-0_conv1') \n",
    "image = imageio.imread(\"greyscale2.jpg\", as_gray =True)\n",
    "image = image.astype('float32') / 255\n",
    "image = np.expand_dims(image, axis=0)\n",
    "heatmap = icam.compute_heatmap(image)\n",
    "heatmap = cv2.resize(heatmap, (512, 512))\n",
    "\n",
    "image = cv2.imread('greyscale2.jpg')\n",
    "image = cv2.resize(image, (512, 512))\n",
    "print(heatmap.shape, image.shape)\n",
    "\n",
    "(heatmap, output) = icam.overlay_heatmap(heatmap, image, alpha=0.5)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "InvalidArgumentError",
     "evalue": "Shapes of all inputs must match: values[0].shape = [] != values[1].shape = [512,7] [Op:Pack]",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-8b82ed19d14a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mheatmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0micam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_heatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mheatmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-e3db610cfa0b>\u001b[0m in \u001b[0;36mcompute_heatmap\u001b[0;34m(self, image, eps)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mconvOutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# use automatic differentiation to compute the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/autosegmentation/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/autosegmentation/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m   1025\u001b[0m       skip_on_eager=False) as name:\n\u001b[1;32m   1026\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbegin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m       packed_begin, packed_end, packed_strides = (stack(begin), stack(end),\n\u001b[0m\u001b[1;32m   1028\u001b[0m                                                   stack(strides))\n\u001b[1;32m   1029\u001b[0m       if (packed_begin.dtype == dtypes.int64 or\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/autosegmentation/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/autosegmentation/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1411\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m       \u001b[0;31m# If the input is a constant list, it can be converted to a constant op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1413\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1414\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Input list contains non-constant tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/autosegmentation/lib/python3.9/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/autosegmentation/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/autosegmentation/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_autopacking_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1535\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cast_nested_seqs_to_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_autopacking_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"packed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/autosegmentation/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_autopacking_helper\u001b[0;34m(list_or_tuple, dtype, name)\u001b[0m\n\u001b[1;32m   1471\u001b[0m           elems_as_tensors.append(\n\u001b[1;32m   1472\u001b[0m               constant_op.constant(elem, dtype=dtype, name=str(i)))\n\u001b[0;32m-> 1473\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melems_as_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_elems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/autosegmentation/lib/python3.9/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mpack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   6381\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6382\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6383\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6384\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6385\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/autosegmentation/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6939\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6940\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6941\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6942\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/autosegmentation/lib/python3.9/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shapes of all inputs must match: values[0].shape = [] != values[1].shape = [512,7] [Op:Pack]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "fig, ax = plt.subplots(1, 3)\n",
    "\n",
    "ax[0].imshow(heatmap)\n",
    "ax[1].imshow(image)\n",
    "ax[2].imshow(output)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Invalid shape (1, 512, 512) for image data",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-41992ef02a0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ax[0].imshow(heatmap)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/autosegmentation/lib/python3.9/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1359\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/autosegmentation/lib/python3.9/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5607\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5609\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5610\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5611\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/autosegmentation/lib/python3.9/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    707\u001b[0m         if not (self._A.ndim == 2\n\u001b[1;32m    708\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m--> 709\u001b[0;31m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0m\u001b[1;32m    710\u001b[0m                             .format(self._A.shape))\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (1, 512, 512) for image data"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"252.317344pt\" version=\"1.1\" viewBox=\"0 0 380.054687 252.317344\" width=\"380.054687pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-19T11:21:43.242151</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 252.317344 \nL 380.054687 252.317344 \nL 380.054687 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 228.439219 \nL 128.573713 228.439219 \nL 128.573713 10.999219 \nL 30.103125 10.999219 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m10f293e7d2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m10f293e7d2\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0.0 -->\n      <g transform=\"translate(22.151563 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"79.338419\" xlink:href=\"#m10f293e7d2\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0.5 -->\n      <g transform=\"translate(71.386857 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"128.573713\" xlink:href=\"#m10f293e7d2\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 1.0 -->\n      <g transform=\"translate(120.622151 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_4\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m8cc2fc3a1c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m8cc2fc3a1c\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.0 -->\n      <g transform=\"translate(7.2 232.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m8cc2fc3a1c\" y=\"184.951219\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0.2 -->\n      <g transform=\"translate(7.2 188.750437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m8cc2fc3a1c\" y=\"141.463219\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0.4 -->\n      <g transform=\"translate(7.2 145.262437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m8cc2fc3a1c\" y=\"97.975219\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.6 -->\n      <g transform=\"translate(7.2 101.774437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m8cc2fc3a1c\" y=\"54.487219\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.8 -->\n      <g transform=\"translate(7.2 58.286437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m8cc2fc3a1c\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 1.0 -->\n      <g transform=\"translate(7.2 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 228.439219 \nL 30.103125 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 128.573713 228.439219 \nL 128.573713 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 228.439219 \nL 128.573713 228.439219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 10.999219 \nL 128.573713 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 148.267831 168.954513 \nL 246.738419 168.954513 \nL 246.738419 70.483925 \nL 148.267831 70.483925 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"148.267831\" xlink:href=\"#m10f293e7d2\" y=\"168.954513\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.0 -->\n      <g transform=\"translate(140.316268 183.55295)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"197.503125\" xlink:href=\"#m10f293e7d2\" y=\"168.954513\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.5 -->\n      <g transform=\"translate(189.551563 183.55295)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"246.738419\" xlink:href=\"#m10f293e7d2\" y=\"168.954513\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 1.0 -->\n      <g transform=\"translate(238.786857 183.55295)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"148.267831\" xlink:href=\"#m8cc2fc3a1c\" y=\"168.954513\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.00 -->\n      <g transform=\"translate(119.002206 172.753732)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"148.267831\" xlink:href=\"#m8cc2fc3a1c\" y=\"144.336866\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.25 -->\n      <g transform=\"translate(119.002206 148.136085)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"148.267831\" xlink:href=\"#m8cc2fc3a1c\" y=\"119.719219\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.50 -->\n      <g transform=\"translate(119.002206 123.518438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"148.267831\" xlink:href=\"#m8cc2fc3a1c\" y=\"95.101572\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0.75 -->\n      <g transform=\"translate(119.002206 98.90079)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"148.267831\" xlink:href=\"#m8cc2fc3a1c\" y=\"70.483925\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 1.00 -->\n      <g transform=\"translate(119.002206 74.283143)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 148.267831 168.954513 \nL 148.267831 70.483925 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 246.738419 168.954513 \nL 246.738419 70.483925 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 148.267831 168.954513 \nL 246.738419 168.954513 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 148.267831 70.483925 \nL 246.738419 70.483925 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_3\">\n   <g id=\"patch_12\">\n    <path d=\"M 266.432537 228.439219 \nL 364.903125 228.439219 \nL 364.903125 10.999219 \nL 266.432537 10.999219 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_5\">\n    <g id=\"xtick_7\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"266.432537\" xlink:href=\"#m10f293e7d2\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 0.0 -->\n      <g transform=\"translate(258.480974 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"315.667831\" xlink:href=\"#m10f293e7d2\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- 0.5 -->\n      <g transform=\"translate(307.716268 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"364.903125\" xlink:href=\"#m10f293e7d2\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_20\">\n      <!-- 1.0 -->\n      <g transform=\"translate(356.951563 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_6\">\n    <g id=\"ytick_12\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"266.432537\" xlink:href=\"#m8cc2fc3a1c\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_21\">\n      <!-- 0.0 -->\n      <g transform=\"translate(243.529412 232.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"266.432537\" xlink:href=\"#m8cc2fc3a1c\" y=\"184.951219\"/>\n      </g>\n     </g>\n     <g id=\"text_22\">\n      <!-- 0.2 -->\n      <g transform=\"translate(243.529412 188.750437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"266.432537\" xlink:href=\"#m8cc2fc3a1c\" y=\"141.463219\"/>\n      </g>\n     </g>\n     <g id=\"text_23\">\n      <!-- 0.4 -->\n      <g transform=\"translate(243.529412 145.262437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"266.432537\" xlink:href=\"#m8cc2fc3a1c\" y=\"97.975219\"/>\n      </g>\n     </g>\n     <g id=\"text_24\">\n      <!-- 0.6 -->\n      <g transform=\"translate(243.529412 101.774437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_16\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"266.432537\" xlink:href=\"#m8cc2fc3a1c\" y=\"54.487219\"/>\n      </g>\n     </g>\n     <g id=\"text_25\">\n      <!-- 0.8 -->\n      <g transform=\"translate(243.529412 58.286437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_17\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"266.432537\" xlink:href=\"#m8cc2fc3a1c\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_26\">\n      <!-- 1.0 -->\n      <g transform=\"translate(243.529412 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_13\">\n    <path d=\"M 266.432537 228.439219 \nL 266.432537 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path d=\"M 364.903125 228.439219 \nL 364.903125 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path d=\"M 266.432537 228.439219 \nL 364.903125 228.439219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path d=\"M 266.432537 10.999219 \nL 364.903125 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('autosegmentation': conda)"
  },
  "interpreter": {
   "hash": "125bec86af28abc83cf35ecc07aed6c1f98608cb92e6f2b8d1d63cbdf2bcb29e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}